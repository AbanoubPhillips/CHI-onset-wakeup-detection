{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":53666,"databundleVersionId":6589269,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nimport json\nfrom datetime import datetime\nimport joblib\nimport random\nimport math\nimport pyarrow as pa \nimport ctypes\nfrom tqdm.auto import tqdm \nfrom scipy.interpolate import interp1d\nfrom math import pi, sqrt, exp\nimport sklearn,sklearn.model_selection\nimport torch\nfrom torch import nn,Tensor\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\nfrom sklearn.metrics import average_precision_score\nfrom timm.scheduler import CosineLRScheduler\nfrom pyarrow.parquet import ParquetFile\nimport gc\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAIN_DIR = \"/kaggle/input/child-mind-institute-detect-sleep-states/\"\nTRAIN_SERIES = MAIN_DIR + \"train_series.parquet\"\nTRAIN_EVENTS = MAIN_DIR + \"train_events.csv\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class data_reader:\n    def __init__(self):\n        super().__init__()\n        self.names_mapping = {\n            \"train_events\" : {\"path\" : TRAIN_EVENTS, \"is_parquet\" : False, \"has_timestamp\" : True},\n            \"train_series\" : {\"path\" : TRAIN_SERIES, \"is_parquet\" : True, \"has_timestamp\" : True}\n        }\n        self.valid_names = [\"train_events\", \"train_series\"]\n    \n    def cleaning(self, data):\n        \"cleaning function : drop na values\"\n        before_cleaning = len(data)\n        print(\"Number of missing timestamps : \", len(data[data[\"timestamp\"].isna()]))\n        data = data.dropna(subset=[\"timestamp\"])\n        after_cleaning = len(data)\n        print(\"Percentage of removed steps : {:.1f}%\".format(100 * (before_cleaning - after_cleaning) / before_cleaning) )\n        return data\n    \n    @staticmethod\n    def reduce_memory_usage(data):\n        start_mem = data.memory_usage().sum() / 1024**2\n        print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n        for col in data.columns:\n            col_type = data[col].dtype    \n            if col_type != object:\n                c_min = data[col].min()\n                c_max = data[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        data[col] = data[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        data[col] = data[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        data[col] = data[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        data[col] = data[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        data[col] = data[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        data[col] = data[col].astype(np.float32)\n                    else:\n                        data[col] = data[col].astype(np.float64)\n            else:\n                data[col] = data[col].astype('category')\n\n        end_mem = data.memory_usage().sum() / 1024**2\n        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n        return data\n    \n   \n\n    def load_data(self, data_name):\n        data_props = self.names_mapping[data_name]\n        if data_props[\"is_parquet\"]:\n            data = pd.read_parquet(data_props[\"path\"])\n        else:\n            data = pd.read_csv(data_props[\"path\"])\n            \n        gc.collect()\n        print('cleaning')\n        data = self.cleaning(data)\n        gc.collect()\n        data = self.reduce_memory_usage(data)\n        return data","metadata":{"execution":{"iopub.status.busy":"2024-08-25T23:26:45.407521Z","iopub.execute_input":"2024-08-25T23:26:45.407951Z","iopub.status.idle":"2024-08-25T23:26:45.451630Z","shell.execute_reply.started":"2024-08-25T23:26:45.407912Z","shell.execute_reply":"2024-08-25T23:26:45.450336Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"reader = data_reader()\nseries = reader.load_data(data_name=\"train_series\")\nevents = reader.load_data(data_name=\"train_events\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = []\ndata = []\nids = series.series_id.unique()\n\nfor viz_id in tqdm(ids):\n    viz_targets = []\n    viz_events = events[events.series_id == viz_id]\n    v_series = series.loc[(series.series_id == viz_id)].copy().reset_index()\n    \n    v_series['dt'] = pd.to_datetime(v_series.timestamp, format='%Y-%m-%dT%H:%M:%S%z').astype(\"datetime64[ns, UTC-04:00]\")\n    v_series['date'] = v_series['dt'].dt.date\n    \n    #     Extract full day data 17280 step each step has 5 seconds\n    steps_per_day = v_series.groupby(['date'], as_index=False)['step'].count()\n    valid_days = steps_per_day[steps_per_day['step'] == 17280]\n    viz_series = pd.merge(v_series, valid_days[['date']], on=['date'], how='inner')\n    \n    for i in range(len(viz_events) - 1):\n        if viz_events.iloc[i].event == 'onset' and viz_events.iloc[i + 1].event == 'wakeup' and viz_events.iloc[i].night == viz_events.iloc[i + 1].night:\n            start, end = viz_events.timestamp.iloc[i], viz_events.timestamp.iloc[i + 1]\n\n            matching_start_rows = viz_series.loc[viz_series.timestamp == start]\n            matching_end_rows = viz_series.loc[viz_series.timestamp == end]\n            \n            if not matching_start_rows.empty and not matching_end_rows.empty:\n                start_id = matching_start_rows.index.values[0]\n                end_id = matching_end_rows.index.values[0]\n                viz_targets.append((start_id, end_id))\n            else:\n                print(f\"No match found for start timestamp: {start} or end timestamp: {end}\")\n                continue  # Skip this iteration if no match is found\n    \n    targets.append(viz_targets)\n    data.append(viz_series[['anglez', 'enmo', 'step']])\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joblib.dump((targets, data, ids), 'train_data.pkl')\nlen(data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del series\ndel events\ndel targets\ndel data \ndel ids\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constants\nSIGMA = 720  # 12 hours\nSAMPLE_FREQ = 12  # 1 observation per minute\nEPOCHS = 5\nBS = 1\nWARMUP_PROP = 0.2\nTRAIN_PROP = 0.9\nmax_chunk_size = 24*60*12","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalize(y):\n    mean = y[:,0].mean().item()\n    std = y[:,0].std().item()\n    y[:,0] = (y[:,0]-mean)/(std+1e-16)\n    mean = y[:,1].mean().item()\n    std = y[:,1].std().item()\n    y[:,1] = (y[:,1]-mean)/(std+1e-16)\n    return y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CHIDataset(Dataset):\n    def __init__(self, file):\n        self.targets, self.data, self.ids = joblib.load(file)\n\n    def downsample_features(self, feat, window_size):\n        # Ensure feat is of type float64 to prevent overflow\n        feat = feat.astype(np.float64)\n        \n        # Pad the sequence if necessary\n        if len(feat) % window_size != 0:\n            padding = np.zeros(window_size - (len(feat) % window_size), dtype=np.float64) + feat[-1]\n            feat = np.concatenate([feat, padding])\n        \n        # Reshape into (num_windows, window_size)\n        feat = np.reshape(feat, (-1, window_size))\n        \n        # Calculate mean and standard deviation for each window\n        feat_mean = np.mean(feat, axis=1)\n        feat_std = np.std(feat, axis=1)\n        \n        # Stack the mean and standard deviation features\n        return np.vstack((feat_mean, feat_std)).T\n\n    def __len__(self):\n        return len(self.targets)\n\n    def __getitem__(self, index):\n        X = self.data[index][['anglez', 'enmo']].astype(np.float64)\n        y = self.targets[index]\n\n        # Generate Gaussian target\n        target_guassian = np.zeros((len(X), 2), dtype=np.float64)\n        for s, e in y:\n            st1, st2 = max(0, s-SIGMA//2), s+SIGMA//2+1\n            ed1, ed2 = e-SIGMA//2, min(len(X), e+SIGMA//2+1)\n            target_guassian[st1:st2, 0] = self.gauss()[st1-(s-SIGMA//2):]\n            target_guassian[ed1:ed2, 1] = self.gauss()[:SIGMA+1-((e+SIGMA//2+1)-ed2)]\n        y = target_guassian\n\n        # Fixing SettingWithCopyWarning\n        X.loc[:, 'anglez'] = np.abs(X['anglez'])\n\n        # Downsample features for window sizes 12, 360, and 720\n        features = []\n        max_len = 0\n        for window_size in [12, 360, 720]:\n            for column in ['anglez', 'enmo']:\n                downsampled = self.downsample_features(X[column].values, window_size)\n                features.append(downsampled)\n                max_len = max(max_len, len(downsampled))\n\n        # Ensure all features have the same length by padding\n        for i in range(len(features)):\n            if len(features[i]) < max_len:\n                padding = np.zeros((max_len - len(features[i]), features[i].shape[1]), dtype=np.float64)\n                features[i] = np.vstack((features[i], padding))\n                \n        # Concatenate all features along the last axis\n        X = np.concatenate(features, axis=1)\n\n        # Downsample target\n        y = np.dstack([self.downsample_seq(y[:, i], SAMPLE_FREQ) for i in range(y.shape[1])])[0]\n        y = normalize(torch.from_numpy(y))\n\n        X = torch.from_numpy(X)\n        return X, y\n\n\n    def gauss(self, n=SIGMA, sigma=SIGMA*0.15):\n        # Gaussian distribution function with higher precision\n        r = range(-int(n/2), int(n/2)+1)\n        return np.array([1 / (sigma * np.sqrt(2*np.pi)) * np.exp(-float(x)**2/(2*sigma**2)) for x in r], dtype=np.float64)\n\n    def downsample_seq(self, feat, downsample_factor=SAMPLE_FREQ):\n        # Downsample data with higher precision\n        feat = feat.astype(np.float64)\n        if len(feat) % downsample_factor != 0:\n            padding = np.zeros(downsample_factor - (len(feat) % downsample_factor), dtype=np.float64) + feat[-1]\n            feat = np.concatenate([feat, padding])\n        feat = np.reshape(feat, (-1, downsample_factor))\n        feat_mean = np.mean(feat, axis=1)\n        return feat_mean","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = CHIDataset('/kaggle/working/train_data.pkl')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResidualBiGRU(nn.Module):\n    def __init__(self, hidden_size, n_layers=1, bidir=True):\n        super(ResidualBiGRU, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n\n        self.gru = nn.GRU(\n            hidden_size,\n            hidden_size,\n            n_layers,\n            batch_first=True,\n            bidirectional=bidir,\n        )\n        \n        dir_factor = 2 if bidir else 1\n        self.fc1 = nn.Linear(hidden_size * dir_factor, hidden_size * dir_factor * 2)\n        self.ln1 = nn.LayerNorm(hidden_size * dir_factor * 2)\n        self.fc2 = nn.Linear(hidden_size * dir_factor * 2, hidden_size)\n        self.ln2 = nn.LayerNorm(hidden_size)\n\n    def forward(self, x, h=None):\n        res, new_h = self.gru(x, h)\n        # res.shape = (batch_size, sequence_size, 2*hidden_size)\n\n        res = self.fc1(res)\n        res = self.ln1(res)\n        res = nn.functional.relu(res)\n\n        res = self.fc2(res)\n        res = self.ln2(res)\n        res = nn.functional.relu(res)\n\n        # skip connection\n        res = res + x\n\n        return res, new_h","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiResidualBiGRU(nn.Module):\n    def __init__(self, input_size, hidden_size, out_size, n_layers, bidir=True):\n        super(MultiResidualBiGRU, self).__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.out_size = out_size\n        self.n_layers = n_layers\n\n        self.fc_in = nn.Linear(input_size, hidden_size)\n        self.ln = nn.LayerNorm(hidden_size)\n        self.res_bigrus = nn.ModuleList(\n            [\n                ResidualBiGRU(hidden_size, n_layers=1, bidir=bidir)\n                for _ in range(n_layers)\n            ]\n        )\n        self.fc_out = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, h=None):\n        if h is None:\n            h = [None for _ in range(self.n_layers)]\n\n        x = self.fc_in(x)\n        x = self.ln(x)\n        x = nn.functional.relu(x)\n\n        new_h = []\n        for i, res_bigru in enumerate(self.res_bigrus):\n            x, new_hi = res_bigru(x, h[i])\n            new_h.append(new_hi)\n\n        x = self.fc_out(x)\n        return x, new_h  # log probabilities + hidden states","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_history(history, model_path=\".\", show=True):\n    epochs = range(1, len(history[\"train_loss\"]) + 1)\n\n    plt.figure()\n    plt.plot(epochs, history[\"train_loss\"], label=\"Training Loss\")\n    plt.plot(epochs, history[\"valid_loss\"], label=\"Validation Loss\")\n    plt.title(\"Loss evolution\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(model_path, \"loss_evo.png\"))\n    if show:\n        plt.show()\n    plt.close()\n\n    plt.figure()\n    plt.plot(epochs, history[\"lr\"])\n    plt.title(\"Learning Rate evolution\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"LR\")\n    plt.savefig(os.path.join(model_path, \"lr_evo.png\"))\n    if show:\n        plt.show()\n    plt.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_size = int(TRAIN_PROP * len(train_ds))\nvalid_size = len(train_ds) - train_size\nindices = torch.randperm(len(train_ds))\ntrain_sampler = SubsetRandomSampler(indices[:train_size])\nvalid_sampler = SubsetRandomSampler(indices[train_size : train_size + valid_size])\nsteps = train_size*EPOCHS\nwarmup_steps = int(steps*WARMUP_PROP)\n\nmodel = MultiResidualBiGRU(input_size=12,hidden_size=64,out_size=2,n_layers=5).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3,weight_decay = 0)\nscheduler = CosineLRScheduler(optimizer,t_initial= steps,warmup_t=warmup_steps, warmup_lr_init=1e-6,lr_min=2e-8)\n\ndt = time.time()\nmodel_path = '/kaggle/working/'\nhistory = {\n    \"train_loss\": [],\n    \"valid_loss\": [],\n    \"valid_mAP\": [],\n    \"lr\": [],\n}\n\nbest_valid_loss = np.inf\ncriterion = torch.nn.MSELoss()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model: nn.Module, max_chunk_size: int, loader: DataLoader, device, criterion):\n    model.eval()\n    valid_loss = 0.0\n    y_true_full = torch.FloatTensor([]).half()\n    y_pred_full = torch.FloatTensor([]).half()\n    for X_batch, y_batch in tqdm(loader, desc=\"Eval\", unit=\"batch\"):\n        # as some of the sequences we are dealing with are pretty long, we use a chunk based approach\n        y_batch = y_batch.to(device, non_blocking=True)\n        pred = torch.zeros(y_batch.shape).to(device, non_blocking=True).half()\n\n        # (re)initialize model's hidden state\n        h = None\n\n        # number of chunks for this sequence (we assume batch size = 1)\n        seq_len = X_batch.shape[1]\n        for i in range(0, seq_len, max_chunk_size):\n            X_chunk = X_batch[:, i : i + max_chunk_size].float().to(device, non_blocking=True)\n\n            y_pred, h = model(X_chunk, h)\n            h = [hi.detach() for hi in h]\n            pred[:, i : i + max_chunk_size] = y_pred.half()\n            del X_chunk\n            gc.collect()\n            \n        loss = criterion(\n            pred.float(),\n            y_batch.float(),\n        )\n        valid_loss += loss.item()\n        del pred,loss\n        gc.collect()\n\n    valid_loss /= len(loader)\n    y_true_full = y_true_full.squeeze(0)\n    y_pred_full = y_pred_full.squeeze(0)\n    gc.collect()\n    return valid_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(\n    train_ds,\n    batch_size=BS,\n    sampler=train_sampler,\n    pin_memory=True,\n    num_workers=1,\n)\nvalid_loader = DataLoader(\n    train_ds,\n    batch_size=1,\n    sampler=valid_sampler,\n    pin_memory=True,\n    num_workers=1,\n)\nfor epoch in range(1, EPOCHS + 1):\n    train_loss = 0.0\n    n_tot_chunks = 0\n    pbar = tqdm(\n        train_loader, desc=\"Training\", unit=\"batch\"\n    )\n    model.train()\n    for step,(X_batch, y_batch) in enumerate(pbar):\n        y_batch = y_batch.to(device, non_blocking=True)\n        pred = torch.zeros(y_batch.shape).to(device, non_blocking=True)\n        optimizer.zero_grad()\n        scheduler.step(step+train_size*epoch)\n        h = None\n\n        seq_len = X_batch.shape[1]\n        for i in range(0, seq_len, max_chunk_size):\n            X_chunk = X_batch[:, i : i + max_chunk_size].float()\n\n            X_chunk = X_chunk.to(device, non_blocking=True)\n\n            y_pred, h = model(X_chunk, h)\n            h = [hi.detach() for hi in h]\n            pred[:, i : i + max_chunk_size] = y_pred\n            del X_chunk,y_pred\n\n        loss = criterion(\n            normalize(pred).float(),\n            y_batch.float(),\n        )\n        loss.backward()\n        train_loss += loss.item()\n        n_tot_chunks+=1\n        pbar.set_description(f'Training: loss = {(train_loss/n_tot_chunks):.2f}')\n\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1e-1)\n        optimizer.step()\n        del pred,loss,y_batch,X_batch,h\n        gc.collect()\n    train_loss /= len(train_loader)\n    del pbar\n    gc.collect()\n    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n\n    if epoch % 1 == 0:\n        valid_loss = evaluate(\n            model, max_chunk_size, valid_loader, device, criterion\n        )\n\n        history[\"train_loss\"].append(train_loss)\n        history[\"valid_loss\"].append(valid_loss)\n        history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(\n                model.state_dict(),\n                os.path.join(model_path, f\"model_best.pth\"),\n            )\n\n        dt = time.time() - dt\n        print(\n            f\"{epoch}/{EPOCHS} -- \",\n            f\"train_loss = {train_loss:.6f} -- \",\n            f\"valid_loss = {valid_loss:.6f} -- \",\n            f\"time = {dt:.6f}s\",\n        )\n        dt = time.time()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history, model_path=model_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_path = os.path.join(model_path, \"history.json\")\nwith open(history_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(history, f, ensure_ascii=False, indent=4)","metadata":{},"execution_count":null,"outputs":[]}]}