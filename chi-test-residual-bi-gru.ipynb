{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":53666,"databundleVersionId":6589269,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nimport json\nfrom datetime import datetime\nimport joblib\nimport random\nimport math\nimport pyarrow as pa \nimport ctypes\nfrom tqdm.auto import tqdm \nfrom scipy.interpolate import interp1d\nfrom math import pi, sqrt, exp\nimport sklearn,sklearn.model_selection\nimport torch\nfrom torch import nn,Tensor\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\nfrom sklearn.metrics import average_precision_score\nfrom timm.scheduler import CosineLRScheduler\nfrom pyarrow.parquet import ParquetFile\nimport gc\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAIN_DIR = \"/kaggle/input/child-mind-institute-detect-sleep-states/\"\nTEST_SERIES = MAIN_DIR + \"test_series.parquet\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class data_reader:\n    def __init__(self):\n        super().__init__()\n        self.names_mapping = {\n            \"test_series\" : {\"path\" : TEST_SERIES, \"is_parquet\" : True, \"has_timestamp\" : True}\n        }\n        self.valid_names = [\"test_series\"]\n    \n    def cleaning(self, data):\n        \"cleaning function : drop na values\"\n        before_cleaning = len(data)\n        print(\"Number of missing timestamps : \", len(data[data[\"timestamp\"].isna()]))\n        data = data.dropna(subset=[\"timestamp\"])\n        after_cleaning = len(data)\n        print(\"Percentage of removed steps : {:.1f}%\".format(100 * (before_cleaning - after_cleaning) / before_cleaning) )\n        return data\n\n    @staticmethod\n    def reduce_memory_usage(data):\n        start_mem = data.memory_usage().sum() / 1024**2\n        print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n        for col in data.columns:\n            col_type = data[col].dtype    \n            if col_type != object:\n                c_min = data[col].min()\n                c_max = data[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        data[col] = data[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        data[col] = data[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        data[col] = data[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        data[col] = data[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        data[col] = data[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        data[col] = data[col].astype(np.float32)\n                    else:\n                        data[col] = data[col].astype(np.float64)\n            else:\n                data[col] = data[col].astype('category')\n\n        end_mem = data.memory_usage().sum() / 1024**2\n        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n        return data\n    \n   \n\n    def load_data(self, data_name):\n        data_props = self.names_mapping[data_name]\n        if data_props[\"is_parquet\"]:\n            data = pd.read_parquet(data_props[\"path\"])\n        else:\n            data = pd.read_csv(data_props[\"path\"])\n            \n        gc.collect()\n        print('cleaning')\n        data = self.cleaning(data)\n        gc.collect()\n        data = self.reduce_memory_usage(data)\n        return data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reader = data_reader()\ntest_series = reader.load_data(data_name=\"test_series\")\nids = test_series.series_id.unique()\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResidualBiGRU(nn.Module):\n    def __init__(self, hidden_size, n_layers=1, bidir=True):\n        super(ResidualBiGRU, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n\n        self.gru = nn.GRU(\n            hidden_size,\n            hidden_size,\n            n_layers,\n            batch_first=True,\n            bidirectional=bidir,\n        )\n        dir_factor = 2 if bidir else 1\n        self.fc1 = nn.Linear(\n            hidden_size * dir_factor, hidden_size * dir_factor * 2\n        )\n        self.ln1 = nn.LayerNorm(hidden_size * dir_factor * 2)\n        self.fc2 = nn.Linear(hidden_size * dir_factor * 2, hidden_size)\n        self.ln2 = nn.LayerNorm(hidden_size)\n\n    def forward(self, x, h=None):\n        res, new_h = self.gru(x, h)\n        # res.shape = (batch_size, sequence_size, 2*hidden_size)\n\n        res = self.fc1(res)\n        res = self.ln1(res)\n        res = nn.functional.relu(res)\n\n        res = self.fc2(res)\n        res = self.ln2(res)\n        res = nn.functional.relu(res)\n\n        # skip connection\n        res = res + x\n\n        return res, new_h","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiResidualBiGRU(nn.Module):\n    def __init__(self, input_size, hidden_size, out_size, n_layers, bidir=True):\n        super(MultiResidualBiGRU, self).__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.out_size = out_size\n        self.n_layers = n_layers\n\n        self.fc_in = nn.Linear(input_size, hidden_size)\n        self.ln = nn.LayerNorm(hidden_size)\n        self.res_bigrus = nn.ModuleList(\n            [\n                ResidualBiGRU(hidden_size, n_layers=1, bidir=bidir)\n                for _ in range(n_layers)\n            ]\n        )\n        self.fc_out = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, h=None):\n        # if we are at the beginning of a sequence (no hidden state)\n        if h is None:\n            # (re)initialize the hidden state\n            h = [None for _ in range(self.n_layers)]\n\n        x = self.fc_in(x)\n        x = self.ln(x)\n        x = nn.functional.relu(x)\n\n        new_h = []\n        for i, res_bigru in enumerate(self.res_bigrus):\n            x, new_hi = res_bigru(x, h[i])\n            new_h.append(new_hi)\n\n        x = self.fc_out(x)\n#         x = F.normalize(x,dim=0)\n        return x, new_h  # log probabilities + hidden states","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constant\nSAMPLE_FREQ = 12\nmax_chunk_size = 24*60*12\nmin_interval = 30","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CHIDataset(Dataset):\n    def __init__(self, series_ids, series):\n        self.series_ids = series_ids\n        self.series = series.reset_index()\n        self.data = []\n\n        # Load and reset index for each series ID\n        for viz_id in tqdm(series_ids):\n            self.data.append(series.loc[(series.series_id == viz_id)].copy().reset_index())\n\n    def downsample_seq_generate_features(self, feat, window_size):\n        # Downsample data and generate features\n        if len(feat) % window_size != 0:\n            feat = np.concatenate([feat, np.zeros(window_size - (len(feat) % window_size)) + feat[-1]])\n        feat = np.reshape(feat, (-1, window_size))\n        feat_mean = np.mean(feat, axis=1)\n        feat_std = np.std(feat, axis=1)\n\n        return np.vstack([feat_mean, feat_std]).T\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        # Extract relevant columns and convert to numpy array\n        X = self.data[index][['anglez', 'enmo']].values.astype(np.float32)\n\n        # Apply absolute value transformation to anglez column\n        X[:, 0] = np.abs(X[:, 0])\n\n        # Downsample features with different window sizes\n        features = []\n        max_len = 0\n        for window_size in [12, 360, 720]:\n            for i in range(X.shape[1]):\n                downsampled = self.downsample_seq_generate_features(X[:, i], window_size)\n                features.append(downsampled)\n                max_len = max(max_len, downsampled.shape[0])\n\n        # Ensure all features have the same length by padding\n        for i in range(len(features)):\n            if features[i].shape[0] < max_len:\n                padding = np.zeros((max_len - features[i].shape[0], features[i].shape[1]))\n                features[i] = np.vstack((features[i], padding))\n\n        # Concatenate all features along the last axis\n        X = np.concatenate(features, axis=1)\n\n        # Convert the numpy array to a torch tensor\n        X = torch.from_numpy(X)\n        return X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the dataset for inference\ntest_ds = SleepDataset(test_series.series_id.unique(), test_series)\n# Clear memory\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MultiResidualBiGRU(input_size=12, hidden_size=64, out_size=2, n_layers=5).to(device).eval()\nmodel.load_state_dict(torch.load(f'/kaggle/input/chi-train-residual-bi-gru/model_best.pth', map_location=device))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame()\n\nfor i in range(len(test_ds)):\n    # Ensure the input data is on the correct device\n    X = test_ds[i].half().to(device)\n    \n    seq_len = X.shape[0]\n    h = None\n    pred = torch.zeros((len(X), 2), device=device).half()  # Ensure pred is also on the correct device\n    \n    for j in range(0, seq_len, max_chunk_size):\n        y_pred, h = model(X[j: j + max_chunk_size].float(), h)\n        h = [hi.detach().to(device) for hi in h]  # Ensure hidden states are on the correct device\n        pred[j: j + max_chunk_size] = y_pred.detach()\n        del y_pred\n        gc.collect()\n    \n    del h, X\n    gc.collect()\n    \n    # Move pred back to CPU for numpy operations\n    pred = pred.cpu().numpy()\n    \n    series_id = ids[i]\n    \n    days = len(pred) / (17280 / 12)\n    scores0, scores1 = np.zeros(len(pred), dtype=np.float16), np.zeros(len(pred), dtype=np.float16)\n    \n    for index in range(len(pred)):\n        if pred[index, 0] == max(pred[max(0, index - min_interval):index + min_interval, 0]):\n            scores0[index] = max(pred[max(0, index - min_interval):index + min_interval, 0])\n        if pred[index, 1] == max(pred[max(0, index - min_interval):index + min_interval, 1]):\n            scores1[index] = max(pred[max(0, index - min_interval):index + min_interval, 1])\n    \n    candidates_onset = np.argsort(scores0)[-max(1, round(days)):]\n    candidates_wakeup = np.argsort(scores1)[-max(1, round(days)):]\n    \n    onset = test_ds.data[i][['step']].iloc[np.clip(candidates_onset * 12, 0, len(test_ds.data[i]) - 1)].astype(np.int32)\n    onset['event'] = 'onset'\n    onset['series_id'] = series_id\n    onset['score'] = scores0[candidates_onset]\n    \n    wakeup = test_ds.data[i][['step']].iloc[np.clip(candidates_wakeup * 12, 0, len(test_ds.data[i]) - 1)].astype(np.int32)\n    wakeup['event'] = 'wakeup'\n    wakeup['series_id'] = series_id\n    wakeup['score'] = scores1[candidates_wakeup]\n    \n    submission = pd.concat([submission, onset, wakeup], axis=0)\n    \n    # Clean up\n    del onset, wakeup, candidates_onset, candidates_wakeup, scores0, scores1, pred, series_id\n    gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = submission.sort_values(['series_id', 'step']).reset_index(drop=True)\nsubmission['row_id'] = submission.index.astype(int)\nsubmission['score'] = submission['score'].fillna(submission['score'].mean())\nsubmission = submission[['row_id', 'series_id', 'step', 'event', 'score']]\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}